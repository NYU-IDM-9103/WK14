{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WK14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAEs\n",
    "\n",
    "### Regression + Un-Regression (Encoder + Decoder)\n",
    "\n",
    "- Learn dense representation of patterns in data\n",
    "- Instead of using these for classification, style transfer, etc, learn how to undo these compressions\n",
    "\n",
    "#### Code:\n",
    "- https://avandekleut.github.io/vae/\n",
    "\n",
    "#### Explanation:\n",
    "- https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n",
    "- https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py\n",
    "!wget -q https://github.com/DM-GY-9103-2024F-H/WK14/raw/main/WK14_utils.py\n",
    "\n",
    "# Get Clouds or faces or flowers\n",
    "!wget -qO- https://github.com/DM-GY-9103-2024F-H/9103-utils/releases/latest/download/clouds.tar.gz | tar xz\n",
    "!wget -qO- https://github.com/DM-GY-9103-2024F-H/9103-utils/releases/latest/download/flowers.tar.gz | tar xz\n",
    "!wget -qO- https://github.com/DM-GY-9103-2024F-H/9103-utils/releases/latest/download/metfaces.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from os import listdir, path\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from image_utils import make_image, open_image\n",
    "\n",
    "from WK14_utils import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "  def __init__(self, imgs):\n",
    "    super().__init__()\n",
    "    self.loader_transform = v2.Compose([\n",
    "      v2.Resize(128),\n",
    "      v2.ToImage(),\n",
    "      v2.ConvertImageDtype(torch.float),\n",
    "    ])\n",
    "    self.num_imgs = len(imgs)\n",
    "    self.imgs = self.loader_transform(imgs)\n",
    "    self.imgs = torch.stack(self.imgs, dim=0)[:, :3]\n",
    "    self.imgs = self.imgs.to(mdevice)\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.num_imgs\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.imgs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"./data/image/metfaces\"\n",
    "filenames = sorted([f for f in listdir(img_dir) if f.endswith(\"png\") or f.endswith(\"jpg\")])\n",
    "\n",
    "images = []\n",
    "for fname in filenames[:50]:\n",
    "  img = open_image(path.join(img_dir, fname))\n",
    "  images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ImageDataset(images)\n",
    "images_dl = DataLoader(ids, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1010)\n",
    "imgs = next(iter(images_dl))\n",
    "display(v2.ToPILImage()(imgs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "  def __init__(self, in_features, hidden_features=512, latent_features=64):\n",
    "    super().__init__()\n",
    "    self.img2hid = nn.Linear(in_features, hidden_features)\n",
    "    self.hid2mean = nn.Linear(hidden_features, latent_features)\n",
    "    self.hid2std = nn.Linear(hidden_features, latent_features)\n",
    "    self.z2hid = nn.Linear(latent_features, hidden_features)\n",
    "    self.hid2img = nn.Linear(hidden_features, in_features)\n",
    "    self.N = torch.distributions.Normal(Tensor([0]).to(mdevice), Tensor([1]).to(mdevice))\n",
    "    self.kl = 0\n",
    "\n",
    "  def encode(self, x):\n",
    "    x = torch.flatten(x, start_dim=1)\n",
    "    hid = F.relu(self.img2hid(x))\n",
    "    mean = self.hid2mean(hid)\n",
    "    std = torch.exp(self.hid2std(hid))\n",
    "    z = mean + std * self.N.sample(mean.shape)\n",
    "    self.kl = (std**2 + mean**2 - torch.log(std) - 0.5).sum()\n",
    "    return z\n",
    "\n",
    "  def decode(self, x):\n",
    "    hid = F.relu(self.z2hid(x))\n",
    "    img = torch.sigmoid(self.hid2img(hid))\n",
    "    return img.reshape(-1, 3, 128, 128)\n",
    "\n",
    "  def forward(self, x):\n",
    "    z = self.encode(x)\n",
    "    return self.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = VAE(in_features=128*128*3).to(mdevice)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "torch.manual_seed(1010)\n",
    "imgs = next(iter(images_dl))\n",
    "y = model(imgs)\n",
    "\n",
    "print(\"Input shape:\", imgs.shape)\n",
    "print(\"Output shape:\", y.shape)\n",
    "print(\"Parameters:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for imgs in images_dl:\n",
    "    optim.zero_grad()\n",
    "    y = model(imgs)\n",
    "    rec_loss = ((imgs - y)**2).sum()\n",
    "    loss= rec_loss + model.kl\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f} rec loss: {rec_loss.item():.4f} kl: {model.kl.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  torch.manual_seed(1010)\n",
    "  imgs = next(iter(images_dl))\n",
    "  y = model(imgs)\n",
    "  idx = 3\n",
    "  display(v2.ToPILImage()(imgs[idx]))\n",
    "  display(v2.ToPILImage()(y[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  z = torch.randn((16,64)).to(mdevice)\n",
    "  ys = model.decode(z)\n",
    "  for y in ys:\n",
    "    display(v2.ToPILImage()(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(v2.ToPILImage()(imgs[1]))\n",
    "display(v2.ToPILImage()(imgs[4]))\n",
    "display(v2.ToPILImage()(imgs[3]))\n",
    "\n",
    "with torch.no_grad():\n",
    "  z = model.encode(imgs)\n",
    "  z = z[1] - z[4] + z[3]\n",
    "  y = model.decode(z)\n",
    "  display(v2.ToPILImage()(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Next Steps\n",
    "\n",
    "- Use CNN to encode/decode images\n",
    "- Look at pre-conditioned VAEs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
