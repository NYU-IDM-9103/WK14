{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WK14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs\n",
    "\n",
    "### Classification + Classification + Classification + ...\n",
    "\n",
    "- Output of NN becomes an input for next prediction\n",
    "- This allows us to have inputs of different lengths\n",
    "\n",
    "#### Code:\n",
    "- https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/\n",
    "\n",
    "#### Explanation:\n",
    "- https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/\n",
    "- https://machinelearningmastery.com/models-sequence-prediction-recurrent-neural-networks/\n",
    "- https://medium.com/@prudhviraju.srivatsavaya/lstm-vs-gru-c1209b8ecb5a\n",
    "- https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "- https://www.kaggle.com/code/dota2player/next-word-prediction-with-lstm-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/WK14/raw/main/WK14_utils.py\n",
    "\n",
    "!wget -q -P ./data/text https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/datasets/text/dickinson.txt\n",
    "!wget -qO- https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/datasets/text/rappers.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from random import choice\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from WK14_utils import TextSequenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDataset(TextSequenceDataset):\n",
    "  def __init__(self, text, max_words=200_000, window=2):\n",
    "    super().__init__(text, max_words, window, symmetric_context=False)\n",
    "    self.words_t = self.encode(self.words)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.words) - self.window\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    target = self.words_t[idx + self.window]\n",
    "    context = self.words_t[idx : idx + self.window]\n",
    "    return context, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/text/dickinson.txt\", \"r\") as f:\n",
    "  dickinson_text = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df = pd.read_csv(\"./data/text/rappers.csv\")\n",
    "rapper_text = lyrics_df[\"lyric\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NGramDataset(text=dickinson_text, max_words=500_000, window=5)\n",
    "train_dl = DataLoader(dataset, batch_size=4096, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWordGRU(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim=64, hidden_dim=256, num_layers=2):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "  def forward(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    out, hidden = self.gru(x, hidden)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = NextWordGRU(vocab_size=len(dataset.wtoi), embedding_dim=64, hidden_dim=256, num_layers=2).to(mdevice)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "input, target = next(iter(train_dl))\n",
    "print(input.shape, target.shape)\n",
    "\n",
    "output, hidden = model(input, None)\n",
    "print(output.shape, hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for input, target in train_dl:\n",
    "    optim.zero_grad()\n",
    "    hidden = None\n",
    "    output, hidden = model(input, hidden)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dataset.encode([\"you\"]).unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  output, _ = model(query, None)\n",
    "  output = output.squeeze()\n",
    "  top1 = output.argmax()\n",
    "  top5 = output.argsort(descending=True)[:5]\n",
    "  print(dataset.decode_word(top1))\n",
    "  print(dataset.decode(top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = [\"You\"]\n",
    "hidden = None\n",
    "num_candidates = 2\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for w in range(10):\n",
    "    query = dataset.encode(phrase).unsqueeze(0)\n",
    "    output, hidden = model(query, None)\n",
    "    output = output.squeeze()\n",
    "    candidates = dataset.decode(output.argsort(descending=True)[:num_candidates])\n",
    "    phrase.append(choice(candidates))\n",
    "\n",
    "print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Next Steps\n",
    "\n",
    "- Implement a translator between two text corpus\n",
    "  - Get a phrase from one text: [W0, W1, W2, etc]\n",
    "  - Get list of embeddings and their relative distances/directions\n",
    "  - Find embedding of W0 in second dataset\n",
    "  - Follow the directions from first dataset, while moving around the second dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
