{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WK14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Transfer\n",
    "\n",
    "### Classification + Classification\n",
    "\n",
    "- Train one CNN for style\n",
    "- Train another CNN for content\n",
    "- Drop classification layers, and use last layer of the CNN to get dense feature representation of images\n",
    "- Given $2$ images (`style` and `content`), get their dense feature representations (`style-f` and `content-f`) by running them through the corresponding CNN\n",
    "- Change the pixels of the `content` image to decrease its difference in relation to the `style-f` and `content-f` representations\n",
    "\n",
    "#### Code:\n",
    "- https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
    "\n",
    "#### Explanation:\n",
    "- https://github.com/Adi-iitd/AI-Art/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py\n",
    "!wget -q https://github.com/DM-GY-9103-2024F-H/WK14/raw/main/WK14_utils.py\n",
    "\n",
    "!wget -P ./data/image -q https://pytorch.org/tutorials/_static/img/neural-style/picasso.jpg\n",
    "!wget -P ./data/image -q https://pytorch.org/tutorials/_static/img/neural-style/dancing.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from image_utils import open_image\n",
    "\n",
    "from WK14_utils import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img = open_image(\"./data/image/dancing.jpg\")\n",
    "style_img = open_image(\"./data/image/picasso.jpg\")\n",
    "\n",
    "display(content_img)\n",
    "display(style_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_transform = v2.Compose([\n",
    "  v2.Resize(512),\n",
    "  v2.ToImage(),\n",
    "  v2.ConvertImageDtype(torch.float)\n",
    "])\n",
    "\n",
    "content_t = loader_transform(content_img).unsqueeze(0)\n",
    "style_t = loader_transform(style_img).unsqueeze(0)\n",
    "\n",
    "print(content_t.shape)\n",
    "print(style_t.shape)\n",
    "\n",
    "display(v2.ToPILImage()(content_t.squeeze()))\n",
    "display(v2.ToPILImage()(style_t.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "image_net_mean = [0.485, 0.456, 0.406]\n",
    "image_net_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "  def __init__(self, mean=image_net_mean, std=image_net_std):\n",
    "    super().__init__()\n",
    "    self.mean = Tensor(mean).reshape(-1, 1, 1).to(mdevice)\n",
    "    self.std = Tensor(std).reshape(-1, 1, 1).to(mdevice)\n",
    "\n",
    "  def forward(self, input):\n",
    "    return (input - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "  def __init__(self, target):\n",
    "    super().__init__()\n",
    "    # we \"detach\" the target content from the computation tree\n",
    "    # since it\"s a fixed value and we don\"t need its cost/slope\n",
    "    self.target = target.detach()\n",
    "\n",
    "  def forward(self, input):\n",
    "    self.loss = F.mse_loss(input, self.target)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "    @staticmethod\n",
    "    def gram_matrix(input):\n",
    "      a, b, c, d = input.size()  # a=batch size(=1)\n",
    "      # b=number of feature maps\n",
    "      # (c,d)=dimensions of a feature map (N=c*d)\n",
    "      features = input.view(a * b, c * d)\n",
    "      G = torch.mm(features, features.t())\n",
    "\n",
    "      # \"normalize\" the values of the gram matrix\n",
    "      return G.div(a * b * c * d)\n",
    "\n",
    "    def __init__(self, target_feature):\n",
    "        super().__init__()\n",
    "        self.target = StyleLoss.gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = StyleLoss.gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "\n",
    "print(count_parameters(model_resnet))\n",
    "display(model_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = vgg19(weights=VGG19_Weights.DEFAULT).features.eval()\n",
    "\n",
    "print(count_parameters(model_vgg))\n",
    "display(model_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_content_layers = [\"conv_4\"]\n",
    "default_style_layers = [\"conv_1\", \"conv_2\", \"conv_3\", \"conv_4\", \"conv_5\"]\n",
    "\n",
    "def create_model(cnn, content, style, content_layers=default_content_layers, style_layers=default_style_layers):\n",
    "  content_losses = []\n",
    "  style_losses = []\n",
    "\n",
    "  model = nn.Sequential(Normalization())\n",
    "\n",
    "  i = 0\n",
    "  # iterate over cnn and copy specific layers\n",
    "  for layer in cnn.children():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "      i += 1\n",
    "      name = f\"conv_{i}\"\n",
    "    elif isinstance(layer, nn.ReLU):\n",
    "      name = f\"relu_{i}\"\n",
    "      layer = nn.ReLU(inplace=False)\n",
    "    elif isinstance(layer, nn.MaxPool2d):\n",
    "      name = f\"pool_{i}\"\n",
    "    elif isinstance(layer, nn.BatchNorm2d):\n",
    "      name = f\"bn_{i}\"\n",
    "    else:\n",
    "      raise RuntimeError(f\"Unrecognized layer: {layer.__class__.__name__}\")\n",
    "\n",
    "    model.add_module(name, layer)\n",
    "\n",
    "    # Add Loss Layers\n",
    "    if name in content_layers:\n",
    "      target = model(content).detach()\n",
    "      content_loss = ContentLoss(target)\n",
    "      model.add_module(f\"content_loss_{i}\", content_loss)\n",
    "      content_losses.append(content_loss)\n",
    "\n",
    "    if name in style_layers:\n",
    "      target_feature = model(style).detach()\n",
    "      style_loss = StyleLoss(target_feature)\n",
    "      model.add_module(f\"style_loss_{i}\", style_loss)\n",
    "      style_losses.append(style_loss)\n",
    "\n",
    "  # Iterate backwards and detect position j of last content/style loss layer\n",
    "  for j in range(len(model) - 1, -1, -1):\n",
    "    if isinstance(model[j], ContentLoss) or isinstance(model[j], StyleLoss):\n",
    "      break\n",
    "  # trim off model to end at last content or style loss\n",
    "  model = model[:(j + 1)]\n",
    "\n",
    "  return model, content_losses, style_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_vgg = model_vgg.to(mdevice)\n",
    "content_t = content_t.to(mdevice)\n",
    "style_t = style_t.to(mdevice)\n",
    "\n",
    "model, content_losses, style_losses = create_model(model_vgg, content_t, style_t)\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "model = model.to(mdevice)\n",
    "\n",
    "input_img = content_t.clone().contiguous().to(mdevice)\n",
    "input_img.requires_grad_(True)\n",
    "optim = torch.optim.LBFGS([input_img], lr=0.1)\n",
    "\n",
    "model(input_img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_weight = 100000\n",
    "content_weight = 1\n",
    "\n",
    "for e in range(100):\n",
    "  def closure():\n",
    "    with torch.no_grad():\n",
    "      input_img.clamp_(0, 1)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    model(input_img)\n",
    "\n",
    "    content_score = 0\n",
    "    style_score = 0\n",
    "    \n",
    "    for cl in content_losses:\n",
    "      content_score += cl.loss\n",
    "    for sl in style_losses:\n",
    "      style_score += sl.loss\n",
    "\n",
    "    content_score *= content_weight  \n",
    "    style_score *= style_weight\n",
    "\n",
    "    loss = style_score + content_score\n",
    "    loss.backward()\n",
    "\n",
    "    return style_score + content_score\n",
    "\n",
    "  score = optim.step(closure)\n",
    "  if e % 10 == 9:\n",
    "    print(f\"Epoch: {e} Score: {score.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  output_img = input_img.squeeze().to(\"cpu\").clamp(0,1)\n",
    "  display(v2.ToPILImage()(output_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Next Steps\n",
    "\n",
    "- Add neighboring pixel difference penalty to loss\n",
    "- Experiment with different combinations of layers in the loss function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
