{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WK14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dream\n",
    "\n",
    "### Classification + Backpropagation\n",
    "\n",
    "- Partially train CNN\n",
    "- Get all partially activated classes at output and figure out which pixels activated them\n",
    "- Change the pixels of the input image to fully activate those classes\n",
    "\n",
    "#### Code:\n",
    "- https://github.com/eriklindernoren/PyTorch-Deep-Dream/blob/master/deep_dream.py\n",
    "\n",
    "#### Explanation:\n",
    "- https://github.com/gordicaleksa/pytorch-deepdream\n",
    "- https://github.com/ProGamerGov/neural-dream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py\n",
    "!wget -q https://github.com/DM-GY-9103-2024F-H/WK14/raw/main/WK14_utils.py\n",
    "\n",
    "!wget -P ./data/image -q https://pytorch.org/tutorials/_static/img/neural-style/picasso.jpg\n",
    "!wget -P ./data/image -q https://pytorch.org/tutorials/_static/img/neural-style/dancing.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from image_utils import make_image, open_image\n",
    "\n",
    "from WK14_utils import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = open_image(\"./data/image/dancing.jpg\")\n",
    "display(original_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mean = Tensor([0.485, 0.456, 0.406])\n",
    "img_std = Tensor([0.229, 0.224, 0.225])\n",
    "img_unmean = img_mean.reshape(3,1,1)\n",
    "img_unstd = img_std.reshape(3,1,1)\n",
    "\n",
    "def unprocess_image(img_t, scale=1):\n",
    "  img_t = img_t * img_unstd + img_unmean\n",
    "  img_t = img_t.squeeze().permute(1, 2, 0)\n",
    "  img_t = scale * img_t.clip(0.0, 1.0)\n",
    "  return img_t\n",
    "\n",
    "def clip_norm(device=\"cpu\"):\n",
    "  img_unmean = img_mean.reshape(3,1,1).to(device)\n",
    "  img_unstd = img_std.reshape(3,1,1).to(device)\n",
    "  def _clip_norm(img_t):\n",
    "    return img_t.clip(-img_unmean / img_unstd, (1 - img_unmean) / img_unstd)\n",
    "  return _clip_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_transform = v2.Compose([\n",
    "  v2.Resize(512),\n",
    "  v2.ToImage(),\n",
    "  v2.ConvertImageDtype(torch.float),\n",
    "  v2.Normalize(img_mean, img_std)\n",
    "])\n",
    "\n",
    "# Create Tensor for NN\n",
    "original_t = process_transform(original_img).unsqueeze(0)\n",
    "print(original_t.shape)\n",
    "\n",
    "# Visualize NN Tensor\n",
    "display(v2.ToPILImage()(original_t.squeeze()))\n",
    "\n",
    "# Visualize un-normalized image\n",
    "display(make_image(unprocess_image(original_t, 255)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_vgg = vgg19(weights=VGG19_Weights.DEFAULT).features.eval()\n",
    "layers = list(model_vgg.children())\n",
    "model = nn.Sequential(*list(model_vgg.children())[:28]).to(mdevice)\n",
    "\n",
    "ClipNorm = clip_norm(mdevice)\n",
    "\n",
    "print(\"full:\", count_parameters(model_vgg), \"dream:\", count_parameters(model))\n",
    "display(model)\n",
    "\n",
    "lr = 1e-2\n",
    "\n",
    "out = model(original_t.to(mdevice))\n",
    "print(out.shape)\n",
    "\n",
    "input_image = original_t.clone().to(mdevice)\n",
    "input_image.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.zero_grad()\n",
    "  out = model(input_image)\n",
    "  loss = out.norm()\n",
    "  loss.backward()\n",
    "\n",
    "  avg_grad = input_image.grad.data.cpu().abs().mean()\n",
    "  norm_lr = lr / avg_grad\n",
    "\n",
    "  input_image.data += norm_lr * input_image.grad.data\n",
    "  input_image.data = ClipNorm(input_image.data)\n",
    "  input_image.grad.data.zero_()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = display(make_image(unprocess_image(input_image.to(\"cpu\"), 255)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Next Steps\n",
    "\n",
    "- Implement octave scaling to add different levels of detail to images\n",
    "- Use custom classifier trained on custom dataset to activates other types of shapes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
